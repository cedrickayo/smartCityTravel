# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME         - Docker image name used to run Airflow.
#                              Default: apache/airflow:master-python3.8
# AIRFLOW_UID                - User ID in Airflow containers
#                              Default: 50000
# AIRFLOW_GID                - Group ID in Airflow containers
#                              Default: 50000
# _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account.
#                              Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account.
#                              Default: airflow
#
# Feel free to modify this file to suit your needs.
---
#version: '3'
#x-airflow-common:
#  &airflow-common
#  build:
#    context: ./airflow #launch the dockerfile
#  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.8.2}
#  environment:
#    &airflow-common-env
#    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#    #AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
#    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
#    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
#    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
#    AIRFLOW__CORE__FERNET_KEY: ''
#    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
#    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
#    AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 100
#
#  volumes:
#    - ./dags:/opt/airflow/dags
#    - ./logs:/opt/airflow/logs
#    - ./plugins:/opt/airflow/plugins
#    - ./jobs:/opt/bitnami/spark/jobs
#    - ./test:/opt/airflow/test
#
#
#  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
#  depends_on:
#    redis:
#      condition: service_healthy
#    postgres:
#      condition: service_healthy
#  networks:
#    - spark-network


services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    hostname: zookeeper
    ports:
      - "2181:2181"
    networks:
      - spark-network



  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka #kafka  if we don't expose listener
      KAFKA_LISTENER_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS:  PLAINTEXT://kafka:9092
    healthcheck:
      test: ["CMD-SHELL", "netstat -anp | grep 9092 || exit 1"]
      interval: 5s
      timeout: 4s
      retries: 7
      start_period: 10s

    networks:
      - spark-network
    expose:
      - 9092

  python-app:
    container_name: application
    build: ./app
    depends_on:
      kafka:
        condition: service_healthy

    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    networks:
      - spark-network



  spark-master:
    image: bitnami/spark:3.5.2
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      SPARK_MODE: "master"
    ports:
      - "8080:8080"
      - "7077:7077"
    depends_on:
      - kafka
    volumes:
      - jobs:/opt/bitnami/spark/jobs #repertoire dans lequel je mettrais mes applications spark
      - spark-data:/opt/bitnami/spark/parquet
      - spark_checkpoints:/opt/bitnami/spark/checkpoint_dir_influx
    networks:
      - spark-network

  spark-worker-1:
      image: bitnami/spark:3.5.2
      container_name: spark-worker-1
      command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      depends_on:
        - "spark-master"
      environment:
        SPARK_MODE: "worker"
        SPARK_MASTER_URL: spark://spark-master:7077
      networks:
        - spark-network
      ports:
        - "8081:8081"
      volumes:
        - jobs:/opt/bitnami/spark/jobs #repertoire dans lequel je mettrais mes applications spark
        - spark-data:/opt/bitnami/spark/parquet
        - spark_checkpoints:/opt/bitnami/spark/checkpoint_dir_influx

  spark-worker-2:
      image: bitnami/spark:3.5.2
      container_name: spark-worker-2
      command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      depends_on:
        - "spark-master"
      environment:
        SPARK_MODE: "worker"
        SPARK_MASTER_URL: spark://spark-master:7077
      networks:
        - spark-network
      ports:
        - "8082:8082"
      volumes:
        - jobs:/opt/bitnami/spark/jobs #repertoire dans lequel je mettrais mes applications spark
        - spark-data:/opt/bitnami/spark/parquet
        - spark_checkpoints:/opt/bitnami/spark/checkpoint_dir_influx

  spark-consumer:
    build:
      context: ./jobs
      dockerfile: Dockerfile #launch the dockerfile
    container_name: spark-consumer
    depends_on:
      - kafka
      - spark-master
      - influxdb
    volumes:
      - jobs:/opt/bitnami/spark/jobs #repertoire dans lequel je mettrais mes applications spark
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077

    env_file:
      - .env

    networks:
      - spark-network


  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
    networks:
      - spark-network

  pgadmin_service:
    image: dpage/pgadmin4
    container_name: my_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: cedric.kayo@gmail.com
      PGADMIN_DEFAULT_PASSWORD: airflow
    ports:
      - "15432:80"
    networks:
      - spark-network
    volumes:
      - pgadmin-data:/var/lib/pgadmin

#  redis:
#    image: redis:latest
#    ports:
#      - 6379:6379
#    healthcheck:
#      test: ["CMD", "redis-cli", "ping"]
#      interval: 5s
#      timeout: 30s
#      retries: 50
#    restart: always
#    networks:
#      - spark-network

#  airflow-webserver:
#    <<: *airflow-common
#    command: webserver
#    container_name: airflow-webserver
#    ports:
#      - 8090:8090 # car Spark master utilise le port 8080 par defaut pour le GUI
#    healthcheck:
#      test: ["CMD", "curl", "--fail", "http://localhost:8090/health"]
#      interval: 10s
#      timeout: 10s
#      retries: 5
#    restart: always
#    networks:
#      - spark-network

#  airflow-scheduler:
#    <<: *airflow-common
#    command: scheduler
#    restart: always
#    container_name: airflow-scheduler
#    healthcheck:
#      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $(hostname) || exit 1"]
#      interval: 10s
#      timeout: 10s
#      retries: 5
#    networks:
#      - spark-network

#  airflow-worker:
#    <<: *airflow-common
#    command: celery worker
#    restart: always
#    container_name: airflow-worker
#    networks:
#      - spark-network

#  airflow-init:
#    <<: *airflow-common
#    command: version
#    environment:
#      <<: *airflow-common-env
#      _AIRFLOW_DB_UPGRADE: 'true'
#      _AIRFLOW_WWW_USER_CREATE: 'true'
#      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
#      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
#    networks:
#      - spark-network

#  metabase:
#    image: metabase/metabase
#    ports:
#      - "3100:3100"
#    networks:
#      - spark-network

  minio:  #Stockage Objet
    image: minio/minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data
    ports:
      - "9000:9000"
    networks:
      - spark-network

#  flower:
#    <<: *airflow-common
#    command: celery flower
#    ports:
#      - 5555:5555
#    healthcheck:
#      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
#      interval: 10s
#      timeout: 10s
#      retries: 5
#    restart: always
#    networks:
#      - spark-network

#  hbase-master:
#    image: blueskyareahm/hbase-base:2.1.3
#    command: master
#    container_name: hbasemaster
#    ports:
#      - "16000:16000"
#      - "16010:16010"
#    depends_on:
#      - zookeeper
#    networks:
#      - spark-network

#  hbase-regionserver:
#    image: blueskyareahm/hbase-base:2.1.3
#    command: regionserver
#    container_name: hbaseregion
#    ports:
#      - "16030:16030"
#      - "16201:16201"
#      - "16301:16301"
#      - "9090:9090"
#    depends_on:
#      - zookeeper
#    networks:
#      - spark-network

#  cassandra:
#    image: cassandra:latest
#    container_name: cassandra
#    ports:
#      - "9042:9042"
#    environment:
#      - CASSANDRA_USER=admin
#      - CASSANDRA_PASSWORD=admin
#    volumes:
#      - cassandra-data:/var/lib/cassandra
#    networks:
#      - spark-network

  influxdb:
    image: influxdb:2.7
    container_name: influxdb
    ports:
      - "8086:8086"
    volumes:
      - influxdb-data:/var/lib/influxdb
      - influxdb-config:/etc/influxdb2
    restart: always
    environment:
      - INFLUXDB_DB=mydb
      - INFLUXDB_ADMIN_USER=${INFLUXDB_ADMIN_USER}
      - INFLUXDB_ADMIN_PASSWORD=${INFLUXDB_ADMIN_PASSWORD}
      - INFLUXDB_TOKEN=${INFLUXDB_TOKEN}
      - INFLUXDB_ORG=${INFLUXDB_ORG}
      - INFLUXDB_BUCKET=${INFLUXDB_BUCKET}
      - INFLUXDB_USER=${INFLUXDB_USER}
      - INFLUXDB_USER_PASSWORD=${INFLUXDB_USER_PASSWORD}
    networks:
      - spark-network


  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana #va utiliser une basse de donnÃ©es sqlite interne pour stocker ses conf(user, password, bd), on pouvait utiliser egalement une bd externe(postgres)
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_ADMIN_PASSWORD}
    restart: unless-stopped
    networks:
      - spark-network


#Kibana et elasticserarch vont ensemble par contre grafana et influx db vont ensemble
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.0 #8.13.2
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.transport.ssl.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    networks:
      - spark-network

  kibana:
    image: docker.elastic.co/kibana/kibana:9.0.0 #8.13.2
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    volumes:
      - kibanadata:/usr/share/kibana/data

    networks:
      - spark-network

networks:
  spark-network:
    driver: bridge

volumes:
  postgres-db-volume:
  cassandra-data:
  pgadmin-data:
  jobs:
  spark-data:
  influxdb-data:
  influxdb-config:
  esdata:
  kibanadata:
  grafana_data:
  spark_checkpoints: